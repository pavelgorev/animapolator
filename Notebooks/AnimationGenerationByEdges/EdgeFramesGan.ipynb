{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Cn_JhaTlz5Sv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22876,
     "status": "ok",
     "timestamp": 1670149432146,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "Cn_JhaTlz5Sv",
    "outputId": "2fe316e9-7a6e-4521-aae1-ca2ce1bc2080"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0FYKIbbnS6lV",
   "metadata": {
    "executionInfo": {
     "elapsed": 3141,
     "status": "ok",
     "timestamp": 1670149437574,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "0FYKIbbnS6lV"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/drive/MyDrive/AI/TextToAnimation/modules /content/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1421a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2814,
     "status": "ok",
     "timestamp": 1670149443999,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "bd1421a3",
    "outputId": "c36df4fc-7617-47f0-9425-21e3650862d2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tcswIyk4vpW9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1670149447271,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "tcswIyk4vpW9",
    "outputId": "1a041a26-8756-4fd2-bdf1-ef6bf268fa5f"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84128fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.TextProcessing import get_description_from, get_text_parameters\n",
    "from modules.DataLoading import LabeledAnimationLoader, create_loaders\n",
    "from modules.PlotAnimation import plot_sample, plot_samples, plot_edge_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a6b1d",
   "metadata": {
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1670153228589,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "a28a6b1d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import arguments\n",
    "\n",
    "# Constants\n",
    "## Data parameters\n",
    "dataset_root = arguments.dataset_root\n",
    "parameters_save_root = arguments.parameters_save_root\n",
    "train_size = arguments.train_size\n",
    "validation_size = arguments.validation_size\n",
    "ignore_size = arguments.ignore_size\n",
    "batch_size = arguments.batch_size\n",
    "\n",
    "image_channels = 3\n",
    "transformed_size = 32\n",
    "\n",
    "## Network parameters\n",
    "embedding_dim = 4\n",
    "# Size is set to one because I wanted to train it on only two animations\n",
    "text_encoder_hidden_size = 1 #720\n",
    "text_encoder_hidden_layers_count = 1\n",
    "\n",
    "hidden_channels = 16\n",
    "\n",
    "## Sequences parameters\n",
    "animation_limit = 5\n",
    "\n",
    "## Training parameters\n",
    "learning_rate = 0.01\n",
    "lambda_recon = 10 #50\n",
    "lambda_seq = 1\n",
    "lambda_frames_count = 0\n",
    "validation_period = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5796f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_set = LabeledAnimationLoader(preprocessed_data_path=dataset_root)\n",
    "\n",
    "# Text tokenizer\n",
    "tokenizer = get_tokenizer('basic_english') # private\n",
    "\n",
    "all_descriptions = data_set.get_descriptions() # private\n",
    "\n",
    "print(all_descriptions)\n",
    "\n",
    "padding_word, encode, count_of_words = get_text_parameters(tokenizer, all_descriptions)\n",
    "\n",
    "for i in range(4):\n",
    "    print(encode(all_descriptions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O67O8_fExrSc",
   "metadata": {
    "executionInfo": {
     "elapsed": 1187,
     "status": "ok",
     "timestamp": 1670153242502,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "O67O8_fExrSc"
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_loaders(data_set, padding_word, batch_size, train_size, validation_size, ignore_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567765df",
   "metadata": {
    "id": "567765df"
   },
   "outputs": [],
   "source": [
    "for n, samples in enumerate(train_loader):\n",
    "    for i in range(batch_size):\n",
    "        print(i)\n",
    "        plot_sample(samples, i)\n",
    "\n",
    "for n, samples in enumerate(val_loader):\n",
    "    plot_samples(samples, validation_size)\n",
    "        \n",
    "#for n, samples in enumerate(val_loader):\n",
    "#    print(f\"{samples['reference'].size() = }\")\n",
    "#    for i in range(validation_size):\n",
    "#        print(i)\n",
    "#        plot_sample(samples, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8AwfNfI_mHB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1670153251692,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "b8AwfNfI_mHB",
    "outputId": "a4dc62e8-4c39-4fc7-95a6-a1c117c6c558",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup GPU training if available\n",
    "\n",
    "device = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829651b6",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670153252975,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "829651b6"
   },
   "outputs": [],
   "source": [
    "# Images Decoder block\n",
    "from modules.ImagesDecoders import InterpolationDecoder\n",
    "\n",
    "# Encoder blocks\n",
    "from modules.Encoders import TextEncoder\n",
    "from modules.ImageNetworkBlocks import ContractingBlock, FeatureMapBlock, FeatureExchangeBlock\n",
    "from modules.ImagesDecoders import ContractingPath, ExpandingPath\n",
    "from modules.Discriminators import ThreeImagesAndDescriptionDiscriminator as Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c80a51",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670153254526,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "91c80a51"
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, count_of_words, embedding_dim, padding_word, h_size, image_channels, hidden_channels, animation_limit):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.text_encoder = TextEncoder(\n",
    "            count_of_words, \n",
    "            embedding_dim, \n",
    "            padding_word, \n",
    "            h_size, \n",
    "            1)\n",
    "        \n",
    "        self.contractingPath = ContractingPath(image_channels, image_channels, hidden_channels, h_size)\n",
    "        \n",
    "        self.featureExchangeStart = FeatureExchangeBlock(2048, (128, 4, 4), h_size)\n",
    "        self.featureExchangeEnd = FeatureExchangeBlock(2048, (128, 4, 4), h_size)\n",
    "        \n",
    "        self.expandingPathStart = ExpandingPath(image_channels, image_channels, hidden_channels, h_size)\n",
    "        self.expandingPathEnd = ExpandingPath(image_channels, image_channels, hidden_channels, h_size)\n",
    "        \n",
    "    def forward(self, description, reference):\n",
    "        h = self.text_encoder(description)\n",
    "        \n",
    "        # Size of h: [1, batches_size, layer_size]. The first dimension is layer index, there is only one layer now.\n",
    "        h = h[0]     \n",
    "        \n",
    "        # Fold -> feature exchange start -> Unfold\n",
    "        #      -> feature exchange end   -> Unfold\n",
    "        \n",
    "        x0, x1, x2, x3 = self.contractingPath(reference, h)\n",
    "        \n",
    "        xfcStart, _ = self.featureExchangeStart(x3, h)\n",
    "        xfcEnd, _ = self.featureExchangeEnd(x3, h)\n",
    "        \n",
    "        start = self.expandingPathStart(x0, x1, x2, xfcStart)\n",
    "        end = self.expandingPathStart(x0, x1, x2, xfcEnd)\n",
    "        \n",
    "        return start, end\n",
    "\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b5900",
   "metadata": {
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1670153262338,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "c66b5900"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def get_gen_loss(\n",
    "    gen, \n",
    "    disc, \n",
    "    start_real,\n",
    "    end_real,\n",
    "    description, \n",
    "    reference,\n",
    "    adv_criterion, \n",
    "    recon_criterion, \n",
    "    lambda_recon,\n",
    "    discription_original=None):\n",
    "    \n",
    "    start_fake, end_fake = gen(description, reference)\n",
    "    evaluation = disc(description, reference, start_fake, end_fake)\n",
    "    adv_loss = adv_criterion(evaluation, torch.ones_like(evaluation))\n",
    "    recon_loss = lambda_recon * (recon_criterion(start_fake, start_real) + recon_criterion(end_fake, end_real))\n",
    "    gen_loss = adv_loss + recon_loss\n",
    "    \n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05d862",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670153264388,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "be05d862"
   },
   "outputs": [],
   "source": [
    "# CHANGE!! +\n",
    "\n",
    "class TrainingData:\n",
    "    def __init__(self, samples):\n",
    "        self.discription = samples['desc_encoded'].to(device)\n",
    "        self.discription_original = samples['description']\n",
    "        images = samples['images'].to(device)\n",
    "        \n",
    "        # NOTE! There could be problems with \"squeeze()\" function. I use it to remove one dimension after picking an image\n",
    "        # from a sequenc. But if there other important dimension with the size of 1, squeezing will also delete it\n",
    "        # TODO: need to rewrite it normally.\n",
    "        self.reference = torch.index_select(images, dim=0, index=torch.tensor([0]).to(device)).squeeze()\n",
    "        self.start_real = torch.index_select(images, dim=0, index=torch.tensor([1]).to(device)).squeeze()\n",
    "        self.end_real = torch.index_select(images, dim=0, index=torch.tensor([2]).to(device)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d79f1",
   "metadata": {
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1670153266915,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "970d79f1"
   },
   "outputs": [],
   "source": [
    "# Display data functions\n",
    "\n",
    "def plot_generated_samples(generator, description_encoded, description, reference, start_frame, end_frame, demos_to_show):\n",
    "    start_fake, end_fake = generator(description_encoded, reference)\n",
    "    \n",
    "    start_fake = start_fake.detach()\n",
    "    end_fake = end_fake.detach()\n",
    "    \n",
    "    print(f'{reference.size() =}')\n",
    "    \n",
    "    plot_edge_frames(reference, start_frame, end_frame, start_fake, end_fake, description, demos_to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c221f",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670153268847,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "fc0c221f"
   },
   "outputs": [],
   "source": [
    "# Network initializing\n",
    "\n",
    "generator = Generator(\n",
    "    count_of_words, \n",
    "    embedding_dim, \n",
    "    padding_word, \n",
    "    text_encoder_hidden_size, \n",
    "    image_channels, \n",
    "    hidden_channels, \n",
    "    animation_limit).to(device)\n",
    "\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "\n",
    "discriminator = Discriminator( \n",
    "    count_of_words, \n",
    "    embedding_dim, \n",
    "    padding_word, \n",
    "    text_encoder_hidden_size, \n",
    "    image_channels,\n",
    "    hidden_channels).to(device)\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=learning_rate * 0.01)\n",
    "\n",
    "adv_criterion = nn.BCEWithLogitsLoss() \n",
    "recon_criterion = nn.L1Loss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd17bd",
   "metadata": {
    "id": "bebd17bd"
   },
   "outputs": [],
   "source": [
    "\n",
    "# parameters_load_root = parameters_save_root#'D:\\AI\\Parameters\\SameImages_3'\n",
    "# discriminator.load_state_dict   (torch.load(os.path.join(parameters_load_root, \"discriminator\"), map_location=torch.device('cpu')))\n",
    "# generator.load_state_dict(torch.load(os.path.join(parameters_load_root, \"generator\"), map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a2c99",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670153271739,
     "user": {
      "displayName": "Павел Горев",
      "userId": "13052752023920836550"
     },
     "user_tz": -60
    },
    "id": "076a2c99"
   },
   "outputs": [],
   "source": [
    "def TrainDiscriminator(samples):\n",
    "    t = TrainingData(samples)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_fake, end_fake = generator(t.discription, t.reference)\n",
    "        start_fake = start_fake.detach()\n",
    "        end_fake = end_fake.detach()\n",
    "\n",
    "    optimizer_discriminator.zero_grad()\n",
    "\n",
    "    disc_fake_hat = discriminator(t.discription, t.reference, start_fake, end_fake)\n",
    "    disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\n",
    "    disc_real_hat = discriminator(t.discription, t.reference, t.start_real, t.end_real)\n",
    "    disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\n",
    "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "    if disc_loss > 0.5:\n",
    "        disc_loss.backward() # Update gradients\n",
    "        optimizer_discriminator.step() # Update optimizer\n",
    "    \n",
    "    return t, disc_loss\n",
    "\n",
    "def TrainGenerator(samples):    \n",
    "    t = TrainingData(samples)\n",
    "\n",
    "    optimizer_generator.zero_grad()\n",
    "    \n",
    "    gen_loss = get_gen_loss(\n",
    "        generator, \n",
    "        discriminator, \n",
    "        t.start_real, \n",
    "        t.end_real, \n",
    "        t.discription, \n",
    "        t.reference,  \n",
    "        adv_criterion, \n",
    "        recon_criterion, \n",
    "        lambda_recon\n",
    "    )                \n",
    "\n",
    "    gen_loss.backward() # Update gradients\n",
    "    optimizer_generator.step() # Update optimizer\n",
    "    \n",
    "    return t, gen_loss\n",
    "\n",
    "def Validate(validation_samples):\n",
    "    val_t = TrainingData(validation_samples)\n",
    "\n",
    "    validation_gen_loss = get_gen_loss(\n",
    "        generator, \n",
    "        discriminator, \n",
    "        val_t.start_real, \n",
    "        val_t.end_real, \n",
    "        val_t.discription, \n",
    "        val_t.reference, \n",
    "        adv_criterion, \n",
    "        recon_criterion, \n",
    "        lambda_recon\n",
    "    ) \n",
    "    \n",
    "    return val_t, validation_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68dfc1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Unnfa0U0a-Not9aEoW98U43IF7xkyl6J"
    },
    "id": "5a68dfc1",
    "outputId": "945cbeed-51d0-4adc-9940-c34657678dc2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "epoch = 0\n",
    "torch.set_printoptions(precision=2, linewidth=200)\n",
    "\n",
    "disc_image_training_iterations = 5 #20\n",
    "gen_training_iterations = 1\n",
    "\n",
    "disc_loss = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Training the discriminators\n",
    "    for _ in range(disc_image_training_iterations):\n",
    "        for n, samples in enumerate(train_loader):\n",
    "            t, disc_loss = TrainDiscriminator(samples)\n",
    "        \n",
    "    # Training the generator\n",
    "    for _ in range(gen_training_iterations):\n",
    "        for n, samples in enumerate(train_loader):\n",
    "            t, gen_loss = TrainGenerator(samples)\n",
    "    \n",
    "    print(f\"[GEN] Epoch: {epoch} Loss D_img.: {disc_loss} Loss G.: {gen_loss}\")#, end = \"\\r\")\n",
    "    \n",
    "    if epoch % validation_period == 0:\n",
    "        \n",
    "        print(f\"Train losses. [GEN] Epoch: {epoch}\")\n",
    "        print(f\"Loss D_img.: {disc_loss:.2f}\")\n",
    "        print(f\"Loss G.: {gen_loss:.2f}.\")\n",
    "            \n",
    "        torch.save(discriminator.state_dict(), os.path.join(parameters_save_root, \"discriminator\"))\n",
    "        torch.save(generator.state_dict(), os.path.join(parameters_save_root, \"generator\"))\n",
    "\n",
    "        validation_samples = next(iter(val_loader))  \n",
    "        val_t, validation_gen_loss = Validate(validation_samples)\n",
    "        \n",
    "        print()\n",
    "        print(f\"[GEN] Epoch: {epoch} Val loss G.: {validation_gen_loss}\")\n",
    "    \n",
    "        # Display training frames\n",
    "        \n",
    "        print(\"=================================\")\n",
    "        print(\"Test samples\")\n",
    "        \n",
    "        # only the last\n",
    "        plot_generated_samples(generator, t.discription, t.discription_original, t.reference, t.start_real, t.end_real, batch_size)\n",
    "        \n",
    "        # Display validation frames\n",
    "        print(\"=================================\")\n",
    "        print(\"Validation samples\")\n",
    "        plot_generated_samples(generator, val_t.discription, val_t.discription_original, val_t.reference, val_t.start_real, val_t.end_real, validation_size)\n",
    "    \n",
    "    epoch = epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7214b9",
   "metadata": {
    "id": "bb7214b9"
   },
   "outputs": [],
   "source": [
    "# torch.save(image_discriminator.state_dict(), os.path.join(parameters_save_root, \"image_discriminator\"))\n",
    "# torch.save(sequence_discriminator.state_dict(), os.path.join(parameters_save_root, \"sequence_discriminator\"))\n",
    "# torch.save(generator.state_dict(), os.path.join(parameters_save_root, \"generator\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97152b84",
   "metadata": {
    "id": "97152b84"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "gan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
